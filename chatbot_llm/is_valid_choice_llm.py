"""
is_valid_choice_llm.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- ì‚¬ìš©ì ì…ë ¥(utterance)ê³¼ í¬ë¡¤ë§ëœ ë°ì´í„°(crawled_data)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
  LLMì—ê²Œ ìœ íš¨í•œ ì„ íƒì§€ë¥¼ íŒë³„í•˜ê³  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
"""

import os
import json
from pathlib import Path
from dotenv import load_dotenv
from openai import AsyncOpenAI

# =====================================================
# í™˜ê²½ ì„¤ì • & OpenAI í´ë¼ì´ì–¸íŠ¸
# =====================================================
load_dotenv()
openai = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

PROJECT_ROOT = Path(__file__).resolve().parent.parent
PROMPT_DIR = PROJECT_ROOT / "prompts"


# =====================================================
# ìœ í‹¸ í•¨ìˆ˜
# =====================================================
def load_text_file(path: Path) -> str:
    """í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ"""
    return path.read_text(encoding="utf-8").strip()


async def _call_is_valid_choice_llm(utterance: str, crawled_data: dict) -> list:
    """
    LLM í˜¸ì¶œ: ì…ë ¥ê³¼ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ íš¨í•œ ì„ íƒ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    try:
        # ğŸ”· í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        crawled_data_str = json.dumps(crawled_data, ensure_ascii=False, indent=2)
        system_prompt = load_text_file(PROMPT_DIR / "is_valid_choice_system_prompt.txt")
        user_prompt = load_text_file(PROMPT_DIR / "is_valid_choice_user_prompt.txt").format(
            utterance=utterance.strip(),
            crawled_data=crawled_data_str
        )

        # ğŸ”· LLM API í˜¸ì¶œ
        response = await openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.0
        )
    except Exception as e:
        print(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return [False, "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤."]

    # ğŸ”· ì‘ë‹µ íŒŒì‹±
    content = response.choices[0].message.content.strip()

    if content.startswith("```"):
        content = content.strip("`").strip()
        if content.lower().startswith(("json", "python")):
            content = content.split("\n", 1)[-1].strip()

    try:
        result = json.loads(content)
        if isinstance(result, list) and len(result) == 2:
            if result[0] is True:
                # ì„±ê³µ
                if isinstance(result[1], list):
                    return [True, result[1]]
            if result[0] is False:
                # ì‹¤íŒ¨
                if isinstance(result[1], str):
                    return [False, result[1]]
    except Exception:
        print("âš ï¸ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")
        print("ì›ë³¸ ì‘ë‹µ:", content)

    # ìµœì¢… ì‹¤íŒ¨ ì²˜ë¦¬
    return [False, "ì£„ì†¡í•©ë‹ˆë‹¤. ì…ë ¥í•˜ì‹  ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ì‹œë„í•´ ì£¼ì„¸ìš”."]


async def is_valid_choice(utterance: str, crawled_data: dict) -> list:
    """
    ì™¸ë¶€ ì§„ì…ì  í•¨ìˆ˜
    """
    return await _call_is_valid_choice_llm(utterance, crawled_data)


# =====================================================
# CLI í…ŒìŠ¤íŠ¸
# =====================================================
if __name__ == "__main__":
    import asyncio

    example_utterance = "ì‚¼ì„±ê³¼ LG ì œí’ˆì´ìš”"
    example_crawled_data = {
        "ì œì¡°ì‚¬": ["ì‚¼ì„±", "LG", "ì• í”Œ"],
        "ë¸Œëœë“œ": ["ê°¤ëŸ­ì‹œ", "ì•„ì´í°"]
    }

    print(f"ì…ë ¥: {example_utterance}")
    output = asyncio.run(is_valid_choice(example_utterance, example_crawled_data))
    print("\nì¶œë ¥:")
    print(output)
